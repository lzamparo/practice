{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: make sure language is consistent: 'labels' is what is to be predicted, 'responses' are the answers to questions on the survey, 'predictors' are questions that are not the labels.\n",
    "\n",
    "survey_data = pd.read_csv(\"https://raw.githubusercontent.com/samtalasila/e-bike-survey-response-results/master/E-Bike_Survey_Responses.csv\")\n",
    "print(survey_data.shape)\n",
    "survey_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_q = 'Does your household have access to any of the following private motorized vehicles?'\n",
    "positive_label = 'No - I do not have access to a private motorized vehicle'\n",
    "\n",
    "labels = survey_data[response_q]\n",
    "predictors = survey_data.drop(columns=[response_q])\n",
    "missing_values_per_predictor = predictors.isna().sum()\n",
    "\n",
    "print(\"Original data was shape: \", survey_data.shape)\n",
    "print(\"Predictors have shape: \", predictors.shape)\n",
    "\n",
    "print(\"Found \", labels.isna().sum(), \" rows with response missing\")\n",
    "print(\"Found \", missing_values_per_predictor.sum(), \" number of missing values total among all predictor columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = predictors.fillna(value='NA')\n",
    "print(\"Found \", len(set(labels)), \"different responses\")\n",
    "\n",
    "positives = 0\n",
    "negatives = 0\n",
    "for label in labels:\n",
    "    if label.strip().startswith(positive_label):\n",
    "        positives += 1\n",
    "    else:\n",
    "        negatives += 1\n",
    "print(\"Got \", positives, \" matches to specific answer\")\n",
    "print(\"Got \", negatives, \" non-mathces to specific answer\")\n",
    "print(\"Expected total of \", labels.shape[0], \" responses, got \", positives + negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_response_complexity_dict = OrderedDict()\n",
    "for k in predictors.keys():\n",
    "    question_response_complexity_dict[k] = \"\"\n",
    "    \n",
    "print(\"Number of questions: \", len(predictors.keys()))  \n",
    "print(\"Cardinality of responses to each survey question:\\n\")    \n",
    "for k in question_response_complexity_dict.keys():\n",
    "    data = predictors[k]\n",
    "    cardinality = len(set(data))\n",
    "    question_response_complexity_dict[k] = cardinality\n",
    "    print(k, \"\\t\", cardinality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go through the predictors, find those answers in each question to be grouped into 'other'\n",
    "# N.B: using a poorly chosen heuristic of 4 is not great, but based on my reading of\n",
    "# the data manually, it seems an okay heuristic. \n",
    "\n",
    "# One of the ways to improve this would be to automatically determine an appropriate threshold via an information theoretic measure\n",
    "# for each predictor, but this is out of scope for a first attempt.\n",
    "\n",
    "replace_as_other = {k: [] for k in predictors.keys()}\n",
    "for k in predictors.keys():\n",
    "    replace_as_other[k] = [n for n, d in predictors[k].value_counts().items() if d < 4]\n",
    "\n",
    "# perform the in-place replacement\n",
    "for k in predictors.keys():\n",
    "    predictors.loc[predictors[k].isin(replace_as_other[k]), k] = 'unusual answer'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Encode labels in the response\n",
    "label_encoder = {positive_label: 1}\n",
    "encoded_labels = [label_encoder.get(r,0) for r in labels]\n",
    "y = np.array(encoded_labels)\n",
    "\n",
    "# Encode the features in the predictors, imputing missing values\n",
    "encoder_pipeline = Pipeline([\n",
    "    (\"vectorizer\", DictVectorizer(sparse=False)), \n",
    "    (\"imputer\", Imputer(missing_values=np.nan, \n",
    "                    strategy=\"mean\",\n",
    "                    axis=0))\n",
    "                    ])\n",
    "\n",
    "features = predictors.to_dict('records')\n",
    "X = encoder_pipeline.fit_transform(features)\n",
    "\n",
    "# Make sure we get sensible features\n",
    "print(\"shape of X: \", X.shape)\n",
    "print(\"type of X: \", type(X))\n",
    "print(\"Any NaN or infinity? \", np.isnan(X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the features in the predictors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "def stemmed_words(words):\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=stemmed_words, stop_words='english', ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = predictors.head()\n",
    "count_data_vec = []\n",
    "tfidf_data_vec = []\n",
    "\n",
    "for name, values in test_data.iteritems():\n",
    "    count_transformed_data = count_vectorizer.fit_transform(values)\n",
    "    count_data_vec.append(count_transformed_data)\n",
    "    tfidf_transformed_data = tfidf_vectorizer.fit_transform(values)\n",
    "    tfidf_data_vec.append(tfidf_transformed_data)\n",
    "\n",
    "print(\"Count data\")\n",
    "print(\"Test data has length: \",len(count_data_vec))\n",
    "print(\"Each element has shape: \", count_data_vec[0].shape)\n",
    "\n",
    "print(\"Tfidf transformed data\")\n",
    "print(\"Test data has length: \", len(tfidf_data_vec))\n",
    "print(\"Each element has shape: \", tfidf_data_vec[0].shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
